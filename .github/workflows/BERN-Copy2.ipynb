{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cabf3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       end_date   yes                                           Headline  \\\n",
      "3455 2022-02-25  41.0  Error: 429 Client Error: Too Many Requests for...   \n",
      "3456 2022-02-25  41.0  Error: 429 Client Error: Too Many Requests for...   \n",
      "3457 2022-02-25  41.0  Error: 429 Client Error: Too Many Requests for...   \n",
      "3458 2022-02-25  41.0  Error: 429 Client Error: Too Many Requests for...   \n",
      "3459 2022-02-25  41.0  Error: 429 Client Error: Too Many Requests for...   \n",
      "3460 2022-02-25  41.0  Error: 429 Client Error: Too Many Requests for...   \n",
      "3461 2022-02-25  41.0  Error: 429 Client Error: Too Many Requests for...   \n",
      "3462 2022-02-25  41.0  Error: 429 Client Error: Too Many Requests for...   \n",
      "3463 2022-02-25  41.0  Error: 429 Client Error: Too Many Requests for...   \n",
      "3464 2022-02-25  41.0  Error: 429 Client Error: Too Many Requests for...   \n",
      "3465 2022-02-25  38.9  Error: 429 Client Error: Too Many Requests for...   \n",
      "3466 2022-02-25  38.9  Error: 429 Client Error: Too Many Requests for...   \n",
      "3467 2022-02-25  38.9  Error: 429 Client Error: Too Many Requests for...   \n",
      "3468 2022-02-25  38.9  Error: 429 Client Error: Too Many Requests for...   \n",
      "3469 2022-02-25  38.9  Error: 429 Client Error: Too Many Requests for...   \n",
      "3470 2022-02-25  38.9  Error: 429 Client Error: Too Many Requests for...   \n",
      "3471 2022-02-25  38.9  Error: 429 Client Error: Too Many Requests for...   \n",
      "3472 2022-02-25  38.9  Error: 429 Client Error: Too Many Requests for...   \n",
      "3473 2022-02-25  38.9  Error: 429 Client Error: Too Many Requests for...   \n",
      "3474 2022-02-25  38.9  Error: 429 Client Error: Too Many Requests for...   \n",
      "3475 2022-02-25  42.6  Error: 429 Client Error: Too Many Requests for...   \n",
      "3476 2022-02-25  42.6  Error: 429 Client Error: Too Many Requests for...   \n",
      "3477 2022-02-25  42.6  Error: 429 Client Error: Too Many Requests for...   \n",
      "3478 2022-02-25  42.6  Error: 429 Client Error: Too Many Requests for...   \n",
      "3479 2022-02-25  42.6  Error: 429 Client Error: Too Many Requests for...   \n",
      "\n",
      "     Published Date  \n",
      "3455            NaN  \n",
      "3456            NaN  \n",
      "3457            NaN  \n",
      "3458            NaN  \n",
      "3459            NaN  \n",
      "3460            NaN  \n",
      "3461            NaN  \n",
      "3462            NaN  \n",
      "3463            NaN  \n",
      "3464            NaN  \n",
      "3465            NaN  \n",
      "3466            NaN  \n",
      "3467            NaN  \n",
      "3468            NaN  \n",
      "3469            NaN  \n",
      "3470            NaN  \n",
      "3471            NaN  \n",
      "3472            NaN  \n",
      "3473            NaN  \n",
      "3474            NaN  \n",
      "3475            NaN  \n",
      "3476            NaN  \n",
      "3477            NaN  \n",
      "3478            NaN  \n",
      "3479            NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your datasets\n",
    "poll = pd.read_csv('president_approval.csv')\n",
    "headlines = pd.read_csv('headlines_f.csv')\n",
    "\n",
    "poll = poll.iloc[:, :2]\n",
    "\n",
    "# Ensure the date columns are in the same format\n",
    "poll['end_date'] = pd.to_datetime(poll['end_date'])\n",
    "headlines['Date'] = pd.to_datetime(headlines['Date'])\n",
    "\n",
    "# Perform the left join\n",
    "merged_df = pd.merge(poll, headlines, left_on='end_date', right_on='Date', how='left')\n",
    "\n",
    "# Remove rows where there's no matching date (i.e., where 'Headline' is null)\n",
    "merged_df = merged_df.dropna(subset=['Headline'])\n",
    "\n",
    "# If you want to keep only one date column, you can drop the redundant one\n",
    "merged_df = merged_df.drop('Date', axis=1)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(merged_df.head(25))\n",
    "\n",
    "# Save the result if needed\n",
    "merged_df.to_csv('merged_poll_headlines.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08be3265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    end_date                                           Headline        yes\n",
      "0 2021-01-21                  Biden Rolls Back the Trump Legacy  51.911667\n",
      "1 2021-01-21  Charlottesville Inspired Biden to Run. Now It ...  51.911667\n",
      "2 2021-01-21  Jen Psaki’s Debut: No Attacks, No Lectures, No...  51.911667\n",
      "3 2021-01-21  The Three Types of Republicans Donald Trump Cr...  51.911667\n",
      "4 2021-01-22                   Biden’s Virus Plans Meet Reality  56.000000\n"
     ]
    }
   ],
   "source": [
    "# Group by 'end_date' and 'Headline' to ensure uniqueness\n",
    "unique_merged_df = (\n",
    "    merged_df\n",
    "    .groupby(['end_date', 'Headline'], as_index=False)\n",
    "    .agg({'yes': 'mean'})  # or any other aggregation function you need\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(unique_merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faa3655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df=unique_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df55206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(810, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200bb1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Python_ISA414Project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['end_date', 'Headline', 'yes'], dtype='object')\n",
      "Shape of the DataFrame: (810, 3)\n",
      "Calculating sentiment scores...\n",
      "Sentiment scores calculated and added to the DataFrame.\n",
      "Updated columns in the DataFrame: Index(['end_date', 'Headline', 'yes', 'sentiment'], dtype='object')\n",
      "Updated shape of the DataFrame: (810, 4)\n",
      "Shape after setting index: (810, 3)\n",
      "Shape of weekly_sentiment: (58,)\n",
      "Shape of weekly_approval: (58,)\n",
      "Shape of combined data: (58, 2)\n",
      "Shape of data after creating lagged features: (56, 4)\n",
      "Mean Squared Error: 18.965501260073992\n",
      "R-squared Score: 0.14783450407941412\n",
      "           feature  importance\n",
      "1  sentiment_lag_1    0.370634\n",
      "2  sentiment_lag_2    0.324555\n",
      "0        sentiment    0.304811\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Function to get sentiment score using BERT\n",
    "def get_sentiment_score(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    scores = outputs[0][0].detach().numpy()\n",
    "    return scores.argmax() + 1  # Returns a score from 1 (very negative) to 5 (very positive)\n",
    "\n",
    "# Print column names and shape of the DataFrame\n",
    "print(\"Columns in the DataFrame:\", merged_df.columns)\n",
    "print(\"Shape of the DataFrame:\", merged_df.shape)\n",
    "\n",
    "# Perform sentiment analysis\n",
    "print(\"Calculating sentiment scores...\")\n",
    "merged_df['sentiment'] = merged_df['Headline'].apply(get_sentiment_score)\n",
    "print(\"Sentiment scores calculated and added to the DataFrame.\")\n",
    "\n",
    "# Print updated column names and shape\n",
    "print(\"Updated columns in the DataFrame:\", merged_df.columns)\n",
    "print(\"Updated shape of the DataFrame:\", merged_df.shape)\n",
    "\n",
    "# Convert date column to datetime\n",
    "date_column = 'end_date'  # Make sure this matches your actual date column name\n",
    "merged_df[date_column] = pd.to_datetime(merged_df[date_column])\n",
    "merged_df.set_index(date_column, inplace=True)\n",
    "\n",
    "print(\"Shape after setting index:\", merged_df.shape)\n",
    "\n",
    "# Aggregate sentiment scores and approval ratings (weekly)\n",
    "weekly_sentiment = merged_df.resample('W')['sentiment'].mean()\n",
    "weekly_approval = merged_df.resample('W')['yes'].mean()\n",
    "\n",
    "print(\"Shape of weekly_sentiment:\", weekly_sentiment.shape)\n",
    "print(\"Shape of weekly_approval:\", weekly_approval.shape)\n",
    "\n",
    "# Combine sentiment and approval data\n",
    "data = pd.concat([weekly_sentiment, weekly_approval], axis=1).dropna()\n",
    "\n",
    "print(\"Shape of combined data:\", data.shape)\n",
    "\n",
    "# Create lagged features\n",
    "for i in range(1, 3):  # Creating 4 weeks of lagged features\n",
    "    data[f'sentiment_lag_{i}'] = data['sentiment'].shift(i)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "print(\"Shape of data after creating lagged features:\", data.shape)\n",
    "\n",
    "# If the dataset is not empty, proceed with modeling\n",
    "if data.shape[0] > 0:\n",
    "    # Prepare features and target\n",
    "    X = data.drop('yes', axis=1)\n",
    "    y = data['yes']\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a Random Forest model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R-squared Score: {r2}\")\n",
    "\n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({'feature': X.columns, 'importance': model.feature_importances_})\n",
    "    print(feature_importance.sort_values('importance', ascending=False))\n",
    "else:\n",
    "    print(\"The dataset is empty after processing.▌\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44a5cf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Python_ISA414Project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['end_date', 'Headline', 'yes'], dtype='object')\n",
      "Shape of the DataFrame: (810, 3)\n",
      "Calculating sentiment scores...\n",
      "Sentiment scores calculated and added to the DataFrame.\n",
      "Updated columns in the DataFrame: Index(['end_date', 'Headline', 'yes', 'sentiment'], dtype='object')\n",
      "Updated shape of the DataFrame: (810, 4)\n",
      "Shape after setting index: (810, 3)\n",
      "Shape of weekly_sentiment: (58,)\n",
      "Shape of weekly_approval: (58,)\n",
      "Shape of combined data: (58, 2)\n",
      "Shape of data after creating lagged features: (57, 3)\n",
      "Mean Squared Error: 18.54528347086191\n",
      "R-squared Score: 0.11533356831674868\n",
      "           feature  importance\n",
      "0        sentiment     0.50129\n",
      "1  sentiment_lag_1     0.49871\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Function to get sentiment score using BERT\n",
    "def get_sentiment_score(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    scores = outputs[0][0].detach().numpy()\n",
    "    return scores.argmax() + 1  # Returns a score from 1 (very negative) to 5 (very positive)\n",
    "\n",
    "# Print column names and shape of the DataFrame\n",
    "print(\"Columns in the DataFrame:\", merged_df.columns)\n",
    "print(\"Shape of the DataFrame:\", merged_df.shape)\n",
    "\n",
    "# Perform sentiment analysis\n",
    "print(\"Calculating sentiment scores...\")\n",
    "merged_df['sentiment'] = merged_df['Headline'].apply(get_sentiment_score)\n",
    "print(\"Sentiment scores calculated and added to the DataFrame.\")\n",
    "\n",
    "# Print updated column names and shape\n",
    "print(\"Updated columns in the DataFrame:\", merged_df.columns)\n",
    "print(\"Updated shape of the DataFrame:\", merged_df.shape)\n",
    "\n",
    "# Convert date column to datetime\n",
    "date_column = 'end_date'  # Make sure this matches your actual date column name\n",
    "merged_df[date_column] = pd.to_datetime(merged_df[date_column])\n",
    "merged_df.set_index(date_column, inplace=True)\n",
    "\n",
    "print(\"Shape after setting index:\", merged_df.shape)\n",
    "\n",
    "# Aggregate sentiment scores and approval ratings (weekly)\n",
    "weekly_sentiment = merged_df.resample('W')['sentiment'].mean()\n",
    "weekly_approval = merged_df.resample('W')['yes'].mean()\n",
    "\n",
    "print(\"Shape of weekly_sentiment:\", weekly_sentiment.shape)\n",
    "print(\"Shape of weekly_approval:\", weekly_approval.shape)\n",
    "\n",
    "# Combine sentiment and approval data\n",
    "data = pd.concat([weekly_sentiment, weekly_approval], axis=1).dropna()\n",
    "\n",
    "print(\"Shape of combined data:\", data.shape)\n",
    "\n",
    "# Create lagged features\n",
    "for i in range(1, 2):  # Creating 4 weeks of lagged features\n",
    "    data[f'sentiment_lag_{i}'] = data['sentiment'].shift(i)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "print(\"Shape of data after creating lagged features:\", data.shape)\n",
    "\n",
    "# If the dataset is not empty, proceed with modeling\n",
    "if data.shape[0] > 0:\n",
    "    # Prepare features and target\n",
    "    X = data.drop('yes', axis=1)\n",
    "    y = data['yes']\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a Random Forest model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R-squared Score: {r2}\")\n",
    "\n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({'feature': X.columns, 'importance': model.feature_importances_})\n",
    "    print(feature_importance.sort_values('importance', ascending=False))\n",
    "else:\n",
    "    print(\"The dataset is empty after processing.▌\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41c5f0",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming merged_df is your original dataframe\n",
    "print(\"Original shape:\", merged_df.shape)\n",
    "\n",
    "# Create a date index (adjust the start date if needed)\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "merged_df.index = pd.date_range(start='2021-01-01', periods=len(merged_df), freq='D')\n",
    "\n",
    "# Create lagged features (only 1 day lag for this small dataset)\n",
    "merged_df['sentiment_lag_1'] = merged_df['sentiment'].shift(1)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data = merged_df.dropna()\n",
    "\n",
    "print(\"Shape after creating lag feature:\", data.shape)\n",
    "\n",
    "# Prepare features and target\n",
    "X = data[['sentiment', 'sentiment_lag_1']]\n",
    "y = data['yes']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared Score: {r2}\")\n",
    "\n",
    "# Print coefficients\n",
    "print(\"\\nModel Coefficients:\")\n",
    "for feature, coef in zip(X.columns, model.coef_):\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "\n",
    "# Print the first few rows of the final dataset\n",
    "print(\"\\nFirst few rows of the final dataset:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf667649",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Print column names and shape of the DataFrame\n",
    "print(\"Columns in the DataFrame:\", merged_df.columns)\n",
    "print(\"Shape of the DataFrame:\", merged_df.shape)\n",
    "\n",
    "# Since we don't have a date column, we'll use the index as our date\n",
    "# Assuming the DataFrame is already sorted by date\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "merged_df.index = pd.date_range(start='2021-01-01', periods=len(merged_df), freq='D')\n",
    "\n",
    "print(\"Shape after setting index:\", merged_df.shape)\n",
    "\n",
    "# Aggregate sentiment scores and approval ratings (weekly)\n",
    "weekly_sentiment = merged_df.resample('W')['sentiment'].mean()\n",
    "weekly_approval = merged_df.resample('W')['yes'].mean()\n",
    "\n",
    "print(\"Shape of weekly_sentiment:\", weekly_sentiment.shape)\n",
    "print(\"Shape of weekly_approval:\", weekly_approval.shape)\n",
    "\n",
    "# Combine sentiment and approval data\n",
    "data = pd.concat([weekly_sentiment, weekly_approval], axis=1).dropna()\n",
    "\n",
    "print(\"Shape of combined data:\", data.shape)\n",
    "\n",
    "# Create lagged features\n",
    "for i in range(1, 5):  # Creating 4 weeks of lagged features\n",
    "    data[f'sentiment_lag_{i}'] = data['sentiment'].shift(i)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "print(\"Shape of data after creating lagged features:\", data.shape)\n",
    "\n",
    "# If the dataset is not empty, proceed with modeling\n",
    "if data.shape[0] > 0:\n",
    "    # Prepare features and target\n",
    "    X = data.drop('yes', axis=1)\n",
    "    y = data['yes']\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a Random Forest model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R-squared Score: {r2}\")\n",
    "\n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({'feature': X.columns, 'importance': model.feature_importances_})\n",
    "    print(feature_importance.sort_values('importance', ascending=False))\n",
    "else:\n",
    "    print(\"The dataset is empty after processing. Please check your data and aggregation steps.\")\n",
    "\n",
    "# Print the first few rows of the final dataset\n",
    "print(\"\\nFirst few rows of the final dataset:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14312c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['end_date', 'Headline', 'yes'], dtype='object')\n",
      "Shape of the DataFrame: (810, 3)\n",
      "Shape after setting index: (810, 2)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/Python_ISA414Project/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/Python_ISA414Project/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/Python_ISA414Project/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentiment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape after setting index:\u001b[39m\u001b[38;5;124m\"\u001b[39m, merged_df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create lagged feature (only 1 day lag for this small dataset)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_lag_1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Drop rows with NaN values\u001b[39;00m\n\u001b[1;32m     24\u001b[0m data \u001b[38;5;241m=\u001b[39m merged_df\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/Python_ISA414Project/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/Python_ISA414Project/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentiment'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Print column names and shape of the DataFrame\n",
    "print(\"Columns in the DataFrame:\", merged_df.columns)\n",
    "print(\"Shape of the DataFrame:\", merged_df.shape)\n",
    "\n",
    "# Convert end_date to datetime\n",
    "merged_df['end_date'] = pd.to_datetime(merged_df['end_date'])\n",
    "merged_df.set_index('end_date', inplace=True)\n",
    "\n",
    "# Sort the DataFrame by date\n",
    "merged_df = merged_df.sort_index()\n",
    "\n",
    "print(\"Shape after setting index:\", merged_df.shape)\n",
    "\n",
    "# Create lagged feature (only 1 day lag for this small dataset)\n",
    "merged_df['sentiment_lag_1'] = merged_df['sentiment'].shift(1)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data = merged_df.dropna()\n",
    "\n",
    "print(\"Shape after creating lag feature:\", data.shape)\n",
    "\n",
    "# Prepare features and target\n",
    "X = data[['sentiment', 'sentiment_lag_1']]\n",
    "y = data['yes']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared Score: {r2}\")\n",
    "\n",
    "# Print coefficients\n",
    "print(\"\\nModel Coefficients:\")\n",
    "for feature, coef in zip(X.columns, model.coef_):\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "\n",
    "# Print the first few rows of the final dataset\n",
    "print(\"\\nFirst few rows of the final dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Print correlation matrix\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(data[['yes', 'sentiment', 'sentiment_lag_1']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d26201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the correct date column name is 'date_column_name'\n",
    "date_column_name = 'end_date'  # Replace with the actual column name\n",
    "\n",
    "# Check if the column exists\n",
    "if date_column_name in merged_df.columns:\n",
    "    # Convert the date column to datetime\n",
    "    merged_df[date_column_name] = pd.to_datetime(merged_df[date_column_name])\n",
    "    merged_df.set_index(date_column_name, inplace=True)\n",
    "\n",
    "    # Sort the DataFrame by date\n",
    "    merged_df = merged_df.sort_index()\n",
    "\n",
    "    print(\"Shape after setting index:\", merged_df.shape)\n",
    "\n",
    "    # Continue with the rest of your analysis...\n",
    "else:\n",
    "    print(f\"Column '{date_column_name}' not found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dcc1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5fe1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python_ISA414Project]",
   "language": "python",
   "name": "conda-env-Python_ISA414Project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
